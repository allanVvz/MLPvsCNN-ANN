# An√°lise T√©cnica Comparativa: MLP vs CNN no MNIST

---

## 1. Multilayer Perceptron (MLP)

### 1.1. Evolu√ß√£o Arquitetural e Hiperpar√¢metros

| Teste | Camadas Ocultas | Ativa√ß√£o            | √âpocas | Otimizador | Batch size | Acur√°cia Valida√ß√£o (%) | Observa√ß√µes                               |
| ----- | --------------- | ------------------- | ------ | ---------- | ---------- | ---------------------- | ----------------------------------------- |
| 1     | 1√ó(n=8)         | sigmoid             | 5      | RMSprop    | 512        | 28.0                   | Underfitting; capacidade insuficiente     |
| 2     | 1√ó(n=50)        | sigmoid             | 5      | RMSprop    | 512        | 79.0                   | Aumento de neur√¥nios ‚Üí ganho expressivo   |
| 4     | 1√ó(n=784)       | softmax / cross‚Äëent | 2      | RMSprop    | 512        | 86.22                  | Transi√ß√£o para softmax + cross‚Äëentropy    |
| 7     | 2√ó(784‚Üí360)     | ReLU                | 5      | RMSprop    | 512        | 92.24                  | ReLU reduz satura√ß√£o, +6 p.p. de acur√°cia |
| 9     | 2√ó(784‚Üí360)     | ReLU                | 10     | RMSprop    | 512        | 96.20                  | +5 √©pocas consolidam aprendizado          |
| 10    | 2√ó(784‚Üí360)     | ReLU                | 30     | RMSprop    | 512        | **96.72**              | Overfitting discreto; performance m√°xima  |

#### Figura 1. Curvas de treinamento e valida√ß√£o do MLP (Teste 10)


### 1.2. An√°lise T√©cnica (Causa e Efeito)

- **Capacidade de Representa√ß√£o:** ampliar de 8‚Üí784 neur√¥nios (Teste¬†1‚Üí4) elevou a acur√°cia de ~27% a 86.2%, saindo do underfitting.
- **Profundidade vs Satura√ß√£o:** inserir segunda camada (n2=360) com ReLU (Teste¬†4‚Üí7) mitigou gradientes saturados do sigmoid, acelerando converg√™ncia em +6 p.p. de acur√°cia.
- **√âpocas de Treino:** estender de 5‚Üí30 √©pocas (Testes¬†7‚Üí10) refinou os pesos; a 10·µÉ √©poca (Teste¬†9) consolidou aprendizado e a 30·µÉ (Teste¬†10) adicionou +0.5 p.p., mas aumentou val_loss (overfitting).
- **Otimizador & Batch:** RMSprop mostrou estabilidade; batches muito pequenos (<64) geram ru√≠do, enquanto 512 equilibra estabilidade e velocidade.

**üîë Conclus√£o MLP:** Melhor configura√ß√£o (Teste¬†10):

- Arquitetura: Dense(784, ReLU) ‚Üí Dense(360, ReLU) ‚Üí Dense(10, Softmax)
- √âpocas: 30 | Batch size: 512 | Otimizador: RMSprop
- Acur√°cia Valida√ß√£o: **96.72%**

---

## 2. Convolutional Neural Network (CNN)

### 2.1. Evolu√ß√£o de Camadas e Hiperpar√¢metros

| Teste | Convs (kernel)     | Filtros C1‚ÜíC2 | Dense p√≥s-Conv | Ativa√ß√£o | √âpocas | % Treino | Otimizador + Perda  | Acur√°cia Valida√ß√£o (%) | Observa√ß√µes                                |
| ----- | ------------------ | ------------- | -------------- | -------- | ------ | -------- | ------------------- | ---------------------- | ------------------------------------------ |
| 1     | 9√ó9 (tanh/sigmoid) | 2‚Üí2           | 2              | tanh     | 2      | 10%      | Adadelta + hinge    | ~9.0                   | Underfitting extremo                       |
| 3     | 3√ó3 (tanh/sigmoid) | 4‚Üí8           | 10             | tanh     | 5      | 10%      | RMSprop + cross-ent | ~25                    | Satura√ß√£o ainda presente                   |
| 6     | 3√ó3 (ReLU)         | 32‚Üí64         | 10             | ReLU     | 10     | 70%      | RMSprop + cross-ent | ~97.8                  | Deep stack + melhora generaliza√ß√£o         |
| 8     | 3√ó3 (ReLU)         | 32‚Üí64         | 256            | ReLU     | 15     | 70%      | RMSprop + cross-ent | **98.61**              | Melhor balan√ßo capacidade vs generaliza√ß√£o |

#### Figura 2. Curvas de treinamento e valida√ß√£o da CNN (Teste 8)


### 2.2. An√°lise T√©cnica (Causa e Efeito)

- **Kernel 3√ó3 vs 9√ó9:** kernels menores preservam detalhes locais, aumentando acur√°cia em ~15 p.p. ao migrar Teste¬†1‚Üí3.
- **Filtros Expandidos:** de 4‚Üí32 e 8‚Üí64 (Testes¬†3‚Üí6) ampliou a extra√ß√£o de features, elevando acur√°cia de ~25% a ~97.8%.
- **Propor√ß√£o de Treino:** 10%‚Üí70% de treino (Teste¬†3‚Üí6) reduziu varia√ß√£o, melhorando generaliza√ß√£o.
- **Batch & √âpocas:** batch 256 + 15 √©pocas (Teste¬†8) refinou padr√µes finos, atingindo 98.61% com curvas est√°veis.

**üîë Conclus√£o CNN:** Melhor configura√ß√£o (Teste¬†8):

- Arquitetura: [Conv3√ó3‚ÜíReLU‚ÜíPool]√ó2 ‚Üí Flatten ‚Üí Dense(256, ReLU) ‚Üí Dense(10, Softmax)
- √âpocas: 15 | Batch size: 256 | Otimizador: RMSprop
- Acur√°cia Valida√ß√£o: **98.61%**

---

## 3. Comparativo T√©cnico MLP vs CNN

| M√©trica             | MLP (Teste¬†10) | CNN (Teste¬†8)   | Diferen√ßa  | Interpreta√ß√£o T√©cnica                            |
| ------------------- | -------------- | --------------- | ---------- | ------------------------------------------------ |
| Acur√°cia Valida√ß√£o  | 96.72%         | 98.61%          | +1.89 p.p. | CNN captura hierarquia espacial de pixels melhor |
| √âpocas Converg√™ncia | 30             | 15              | ‚Äì50%       | CNN converge mais r√°pido por par√¢metros locais   |
| Generaliza√ß√£o       | <60% (Conj.¬†2) | ~70% (Conj.¬†2)  | +10 p.p.   | CNN generaliza melhor em dados fora do treino    |

> **Insight:** Convolu√ß√µes 3√ó3 + pooling permitem √† CNN extrair padr√µes locais antes de combina√ß√µes densas, resultando em acur√°cia superior e melhor generaliza√ß√£o com menos √©pocas, enquanto a MLP requer maior profundidade densa e mais ciclos de treino.

---

*Este documento foi revisado para manter fidelidade ao conte√∫do original, remover marca√ß√µes internas e uniformizar formata√ß√£o e arquitetura.*

